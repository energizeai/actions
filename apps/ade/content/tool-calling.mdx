---
title: "Tool Calling"
group: "Callers"
groupOrder: 200
---

#### LLM Tool Calling

# Tool Calling

<Warning>
Please read the [Action Calling](/action-calling) documentation first. Function calling builds on top of action calling and you still want to familiarize yourself with the powerful features in action calling.
</Warning>

## Calling Tools 

To call tools, pass your action registry to the `setupToolCalling` function. This will return an object with a `tools` and `toolCallsHandler` property.

Send the `tools` property to the LLM.

Send the returned `tool_calls` to the `toolCallsHandler` function.

Done! ðŸŽŠ

```typescript:index.ts
import { setupToolCalling } from "ai-actions"
import OpenAI from "openai"

const openai = new OpenAI()

async function invokeActions(userMessage: string) {
  const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
    {
      role: "user",
      content: userMessage,
    },
  ]

  // setup the tool calling
  const { toolCallsHandler, tools } = setupToolCalling(ActionsRegistry, {}) <|highlight|>

  // call the LLM with the tools
  const responseMessage = await openai.chat.completions.create({
    model: "gpt-3.5-turbo-1106",
    messages,
    tools, // pass the tools <|highlight|>
  })

  // get the tool calls the LLM made
  const choice = responseMessage.choices[0]
  if (!choice || !choice.message.tool_calls) return

  // pass the tool calls to the handler
  const { results, toolCallMessages } = await toolCallsHandler( <|highlight|>
    choice.message.tool_calls <|highlight|>
  ) <|highlight|>

  // a typesafe array of results from the handlers
  console.log(results) <|highlight|>

  // append the assistant message to the messages
  messages.push(choice.message)

  // call the LLM again with the handler outputs
  const secondResponse = await openai.chat.completions.create({
    model: "gpt-3.5-turbo-0125",
    messages: messages.concat(toolCallMessages), // add the tool calls to the messages <|highlight|>
  })

  // log the final response
  console.log(secondResponse.choices[0]?.message.content)
}

invokeActions("What is the stock price for MSFT?")
```

## Few Shot Tool Calls

Few shotting is a technique that allows you to teach the LLM in context by providing examples. 

A common error is providing examples that don't match the schemas of the tool. This problem is solved by using the `createFewShotToolCallMessages` function.

It will force you to provide type-safe arguments and responses that match the schemas of the tool.

<Note>
You can teach the LLM to parallel function call by passing multiple tool calls in response to a single user message.
</Note>

```typescript:index.ts
const { toolCallsHandler, tools, createFewShotToolCallMessages } = setupToolCalling(ActionsRegistry, {})

const fewShotMessages: OpenAI.Chat.ChatCompletionMessageParam[] = createFewShotToolCallMessages([ // pass the few shot tool calls to the handler <|highlight|>
  { <|highlight|>
    userMessageContent: "What is the stock price for MSFT and TSLA?", <|highlight|>
    tool_calls: [ <|highlight|>
      { <|highlight|>
        name: "getStockPrice", <|highlight|>
        arguments: { <|highlight|>
          symbol: "MSFT", <|highlight|>
        }, <|highlight|>
        response: { <|highlight|>
          price: 123, <|highlight|>
        }, <|highlight|>
      }, { <|highlight|>
        name: "getStockPrice", <|highlight|>
        arguments: { <|highlight|>
          symbol: "TSLA", <|highlight|>
        }, <|highlight|>
        response: { <|highlight|>
          price: 456, <|highlight|>
        }, <|highlight|>
      } <|highlight|>
    ], <|highlight|>
    assistantMessageContent: "The stock price for MSFT is $123 and the stock price for TSLA is $456", <|highlight|>
  }, <|highlight|>
]) <|highlight|>

// call the LLM with the tools
const responseMessage = await openai.chat.completions.create({
  model: "gpt-3.5-turbo-1106",
  messages: [
    ...fewShotMessages,
    {
      role: "user",
      content: "What is the stock price for AAPL?", // higher accuracy <|highlight|>
    }
  ],
  tools,
})
```

Here is another example of sequential tool calling:

<Note>
In this example, we teach the model to first get the contact information for Ido and then send an email to him asking what time the meeting is on Friday.
</Note>

```typescript:index.tsx
const { toolCallsHandler, tools, createFewShotToolCallMessages } = setupToolCalling(ActionsRegistry, {})

const fewShotMessages: OpenAI.Chat.ChatCompletionMessageParam[] = createFewShotToolCallMessages([ // pass the few shot tool calls to the handler <|highlight|>
  { <|highlight|>
    userMessageContent: "Can you get Ido's email and send mail to him asking what time the meeting is on Friday?", <|highlight|>
    tool_calls: [ <|highlight|>
      { <|highlight|>
        name: "getContact", <|highlight|>
        arguments: { <|highlight|>
          query: "Ido", <|highlight|>
        }, <|highlight|>
        response: { <|highlight|>
          email: "idoemail@example.com", <|highlight|>
        }, <|highlight|>
      } <|highlight|>
    ], <|highlight|>
  }, <|highlight|>
  { <|highlight|>
    tool_calls: [ <|highlight|>
       { <|highlight|>
        name: "sendEmail", <|highlight|>
        arguments: { <|highlight|>
          email: "idoemail@example.com", <|highlight|>
          content: "Hi Ido, What time is the meeting on Friday?", <|highlight|>
        }, <|highlight|>
        response: { <|highlight|>
          status: "success", <|highlight|>
        }, <|highlight|>
      } <|highlight|>
    ], <|highlight|>
    assistantMessageContent: "I have sent Ido an email asking what time the meeting is on Friday", <|highlight|>
  }, <|highlight|>
]) <|highlight|>

// call the LLM with the tools
const responseMessage = await openai.chat.completions.create({
  model: "gpt-3.5-turbo-1106",
  messages: [
    ...fewShotMessages,
    {
      role: "user",
      content: "Can you send Ethan an email asking what time the meeting is on Friday?", // higher accuracy <|highlight|>
    }
  ],
  tools,
})
```

### Chain of Thought

You can add an optional `thinkingStep` to each index in the few shot array. This will be used to generate the thinking step in the LLM and boost performance.

```typescript:index.ts
const fewShotMessages = createFewShotToolCallMessages([ // pass the few shot tool calls to the handler
  {
    userMessageContent: "What is the stock price for MSFT and TSLA?",
    thinkingStep: "The user asked for the stock price for MSFT and TSLA. I will need to get the stock prices for both of them in parallel using the getStockPrice tool.", <|highlight|>
    tool_calls: [
      {
        name: "getStockPrice",
        arguments: {
          symbol: "MSFT",
        },
        response: {
          price: 123,
        },
      }, {
        name: "getStockPrice",
        arguments: {
          symbol: "TSLA",
        },
        response: {
          price: 456,
        },
      }
    ],
    assistantMessageContent: "The stock price for MSFT is $123 and the stock price for TSLA is $456",
  },
])
```

### Inferring Few Shot

You can also use the `inferFewShotToolCallMessages` helper to infer the few shot tool calls. 

This is useful if you want to define few shot messages outside the scope of `createFewShotToolCallMessages`.

```typescript:index.ts
import { inferFewShotToolCallMessages } from "ai-actions"

export const fewShotMessages: inferFewShotToolCallMessages<
  typeof ActionsRegistry,
  "myActionId" | "myActionId2" // optionally pass in the action ids to infer the few shot tool calls for
> = [
  ...
]
```

## Choose Tool

If you want the LLM to always call a specific action, use `chooseTool`.

```typescript:index.ts
// setup the tool calling
const { toolCallsHandler, tools, chooseTool } = setupToolCalling(ActionsRegistry, {}) <|highlight|>

// call the LLM with the tools
const responseMessage = await openai.chat.completions.create({
  model: "gpt-3.5-turbo-1106",
  messages,
  tools,
  tool_choice: chooseTool("myActionId"), <|highlight|>
})
```